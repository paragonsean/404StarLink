# coding: utf-8

"""
    Dataflow Kit Web Scraper

    Render Javascript driven pages, while we internally manage Headless Chrome and proxies for you.   - Build a custom web scraper with our Visual point-and-click toolkit. - Scrape the most popular Search engines result pages (SERP). - Convert web pages to PDF and capture screenshots. *** ### Authentication Dataflow Kit API require you to sign up for an API key in order to use the API.   The API key can be found in the [DFK Dashboard](https://account.dataflowkit.com) after _free registration_.  Pass a secret API Key to all API requests to the server as the `api_key` query parameter.  

    The version of the OpenAPI document: 1.3
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501

import warnings
from pydantic import validate_call, Field, StrictFloat, StrictStr, StrictInt
from typing import Any, Dict, List, Optional, Tuple, Union
from typing_extensions import Annotated

from pydantic import Field
from typing import Any, Dict
from typing_extensions import Annotated
from openapi_client.models.parserequest import Parserequest

from openapi_client.api_client import ApiClient, RequestSerialized
from openapi_client.api_response import ApiResponse
from openapi_client.rest import RESTResponseType


class ParseApi:
    """NOTE: This class is auto generated by OpenAPI Generator
    Ref: https://openapi-generator.tech

    Do not edit the class manually.
    """

    def __init__(self, api_client=None) -> None:
        if api_client is None:
            api_client = ApiClient.get_default()
        self.api_client = api_client


    @validate_call
    def parse(
        self,
        parserequest: Annotated[Parserequest, Field(description="### Field types and attributes    - **Text**. Extract human-readable text from the selected element and all its child elements. HTML tags are stripped, and only text returned.      - **Link**. Capture link `href` attribute and link text. Or specify a special _Path_ option for website navigation. When Path option is true, all other selectors ignored, and no results from the current page returned.      - **Image**. Image type extracts `src` (URL) and `alt` attributes of an image   *** ### Filters Filters are used to manipulate text data when extracting.  Here is the list of available filters   - **Trim** removes leading and trailing white spaces from the _field text or attribute_  - **Normal** leaves the case and capitalization of text/ attribute exactly as is.  - **UPPERCASE** makes all of the letters in the Field's text/ attribute uppercase.  - **lowercase** makes all of the letters in the Field's text/ attribute lowercase.  - **Capitalize** capitalizes the first letter of each word in the Field's text/ attribute  - **Concatinate** joins text array element into a single string  *** ### Regular Expressions  For more advanced text formatting regular expression can be used. Some useful examples are listed below   | Input text | Regex | Result | | ---------- | ----- | ------ | | price- 10.99€ | <code>[0-9]+\\.[0-9]+</code> | 10.99 | | phone- 0 (944) 244-18-22 | <code>\\w+</code> | 09442441822 |   *** ### Details. Chaining. The Link field type serves as a navigation link to a details page containing more data. A special _Path_ option is used for navigation only. When the Path option specified, no results from the current page returned. But grouped results from details pages will be pulled instead. You can use chaining functionality of Dataflow Kit scraper to retrieve all the detail page data at the same time. ")],
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> object:
        """Extract structured data from web pages

        Dataflow kit uses CSS selectors to find HTML elements in web pages for later data extraction.  Open [visual point-and-click toolkit](https://dataflowkit.com/dfk) and click desired elements on a page to specify extracting data.     Then you can send generated payload to `/parse` endpoint. We crawl web pages and extract data like text, links, or images for you following the specified rules.    Extracted data is returned in CSV, MS Excel, JSON, JSON(Lines) or XML format. 

        :param parserequest: ### Field types and attributes    - **Text**. Extract human-readable text from the selected element and all its child elements. HTML tags are stripped, and only text returned.      - **Link**. Capture link `href` attribute and link text. Or specify a special _Path_ option for website navigation. When Path option is true, all other selectors ignored, and no results from the current page returned.      - **Image**. Image type extracts `src` (URL) and `alt` attributes of an image   *** ### Filters Filters are used to manipulate text data when extracting.  Here is the list of available filters   - **Trim** removes leading and trailing white spaces from the _field text or attribute_  - **Normal** leaves the case and capitalization of text/ attribute exactly as is.  - **UPPERCASE** makes all of the letters in the Field's text/ attribute uppercase.  - **lowercase** makes all of the letters in the Field's text/ attribute lowercase.  - **Capitalize** capitalizes the first letter of each word in the Field's text/ attribute  - **Concatinate** joins text array element into a single string  *** ### Regular Expressions  For more advanced text formatting regular expression can be used. Some useful examples are listed below   | Input text | Regex | Result | | ---------- | ----- | ------ | | price- 10.99€ | <code>[0-9]+\\.[0-9]+</code> | 10.99 | | phone- 0 (944) 244-18-22 | <code>\\w+</code> | 09442441822 |   *** ### Details. Chaining. The Link field type serves as a navigation link to a details page containing more data. A special _Path_ option is used for navigation only. When the Path option specified, no results from the current page returned. But grouped results from details pages will be pulled instead. You can use chaining functionality of Dataflow Kit scraper to retrieve all the detail page data at the same time.  (required)
        :type parserequest: Parserequest
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._parse_serialize(
            parserequest=parserequest,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "object",
            '400': None,
            '401': None,
            '500': None,
        }
        response_data = self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        ).data


    @validate_call
    def parse_with_http_info(
        self,
        parserequest: Annotated[Parserequest, Field(description="### Field types and attributes    - **Text**. Extract human-readable text from the selected element and all its child elements. HTML tags are stripped, and only text returned.      - **Link**. Capture link `href` attribute and link text. Or specify a special _Path_ option for website navigation. When Path option is true, all other selectors ignored, and no results from the current page returned.      - **Image**. Image type extracts `src` (URL) and `alt` attributes of an image   *** ### Filters Filters are used to manipulate text data when extracting.  Here is the list of available filters   - **Trim** removes leading and trailing white spaces from the _field text or attribute_  - **Normal** leaves the case and capitalization of text/ attribute exactly as is.  - **UPPERCASE** makes all of the letters in the Field's text/ attribute uppercase.  - **lowercase** makes all of the letters in the Field's text/ attribute lowercase.  - **Capitalize** capitalizes the first letter of each word in the Field's text/ attribute  - **Concatinate** joins text array element into a single string  *** ### Regular Expressions  For more advanced text formatting regular expression can be used. Some useful examples are listed below   | Input text | Regex | Result | | ---------- | ----- | ------ | | price- 10.99€ | <code>[0-9]+\\.[0-9]+</code> | 10.99 | | phone- 0 (944) 244-18-22 | <code>\\w+</code> | 09442441822 |   *** ### Details. Chaining. The Link field type serves as a navigation link to a details page containing more data. A special _Path_ option is used for navigation only. When the Path option specified, no results from the current page returned. But grouped results from details pages will be pulled instead. You can use chaining functionality of Dataflow Kit scraper to retrieve all the detail page data at the same time. ")],
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> ApiResponse[object]:
        """Extract structured data from web pages

        Dataflow kit uses CSS selectors to find HTML elements in web pages for later data extraction.  Open [visual point-and-click toolkit](https://dataflowkit.com/dfk) and click desired elements on a page to specify extracting data.     Then you can send generated payload to `/parse` endpoint. We crawl web pages and extract data like text, links, or images for you following the specified rules.    Extracted data is returned in CSV, MS Excel, JSON, JSON(Lines) or XML format. 

        :param parserequest: ### Field types and attributes    - **Text**. Extract human-readable text from the selected element and all its child elements. HTML tags are stripped, and only text returned.      - **Link**. Capture link `href` attribute and link text. Or specify a special _Path_ option for website navigation. When Path option is true, all other selectors ignored, and no results from the current page returned.      - **Image**. Image type extracts `src` (URL) and `alt` attributes of an image   *** ### Filters Filters are used to manipulate text data when extracting.  Here is the list of available filters   - **Trim** removes leading and trailing white spaces from the _field text or attribute_  - **Normal** leaves the case and capitalization of text/ attribute exactly as is.  - **UPPERCASE** makes all of the letters in the Field's text/ attribute uppercase.  - **lowercase** makes all of the letters in the Field's text/ attribute lowercase.  - **Capitalize** capitalizes the first letter of each word in the Field's text/ attribute  - **Concatinate** joins text array element into a single string  *** ### Regular Expressions  For more advanced text formatting regular expression can be used. Some useful examples are listed below   | Input text | Regex | Result | | ---------- | ----- | ------ | | price- 10.99€ | <code>[0-9]+\\.[0-9]+</code> | 10.99 | | phone- 0 (944) 244-18-22 | <code>\\w+</code> | 09442441822 |   *** ### Details. Chaining. The Link field type serves as a navigation link to a details page containing more data. A special _Path_ option is used for navigation only. When the Path option specified, no results from the current page returned. But grouped results from details pages will be pulled instead. You can use chaining functionality of Dataflow Kit scraper to retrieve all the detail page data at the same time.  (required)
        :type parserequest: Parserequest
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._parse_serialize(
            parserequest=parserequest,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "object",
            '400': None,
            '401': None,
            '500': None,
        }
        response_data = self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        )


    @validate_call
    def parse_without_preload_content(
        self,
        parserequest: Annotated[Parserequest, Field(description="### Field types and attributes    - **Text**. Extract human-readable text from the selected element and all its child elements. HTML tags are stripped, and only text returned.      - **Link**. Capture link `href` attribute and link text. Or specify a special _Path_ option for website navigation. When Path option is true, all other selectors ignored, and no results from the current page returned.      - **Image**. Image type extracts `src` (URL) and `alt` attributes of an image   *** ### Filters Filters are used to manipulate text data when extracting.  Here is the list of available filters   - **Trim** removes leading and trailing white spaces from the _field text or attribute_  - **Normal** leaves the case and capitalization of text/ attribute exactly as is.  - **UPPERCASE** makes all of the letters in the Field's text/ attribute uppercase.  - **lowercase** makes all of the letters in the Field's text/ attribute lowercase.  - **Capitalize** capitalizes the first letter of each word in the Field's text/ attribute  - **Concatinate** joins text array element into a single string  *** ### Regular Expressions  For more advanced text formatting regular expression can be used. Some useful examples are listed below   | Input text | Regex | Result | | ---------- | ----- | ------ | | price- 10.99€ | <code>[0-9]+\\.[0-9]+</code> | 10.99 | | phone- 0 (944) 244-18-22 | <code>\\w+</code> | 09442441822 |   *** ### Details. Chaining. The Link field type serves as a navigation link to a details page containing more data. A special _Path_ option is used for navigation only. When the Path option specified, no results from the current page returned. But grouped results from details pages will be pulled instead. You can use chaining functionality of Dataflow Kit scraper to retrieve all the detail page data at the same time. ")],
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> RESTResponseType:
        """Extract structured data from web pages

        Dataflow kit uses CSS selectors to find HTML elements in web pages for later data extraction.  Open [visual point-and-click toolkit](https://dataflowkit.com/dfk) and click desired elements on a page to specify extracting data.     Then you can send generated payload to `/parse` endpoint. We crawl web pages and extract data like text, links, or images for you following the specified rules.    Extracted data is returned in CSV, MS Excel, JSON, JSON(Lines) or XML format. 

        :param parserequest: ### Field types and attributes    - **Text**. Extract human-readable text from the selected element and all its child elements. HTML tags are stripped, and only text returned.      - **Link**. Capture link `href` attribute and link text. Or specify a special _Path_ option for website navigation. When Path option is true, all other selectors ignored, and no results from the current page returned.      - **Image**. Image type extracts `src` (URL) and `alt` attributes of an image   *** ### Filters Filters are used to manipulate text data when extracting.  Here is the list of available filters   - **Trim** removes leading and trailing white spaces from the _field text or attribute_  - **Normal** leaves the case and capitalization of text/ attribute exactly as is.  - **UPPERCASE** makes all of the letters in the Field's text/ attribute uppercase.  - **lowercase** makes all of the letters in the Field's text/ attribute lowercase.  - **Capitalize** capitalizes the first letter of each word in the Field's text/ attribute  - **Concatinate** joins text array element into a single string  *** ### Regular Expressions  For more advanced text formatting regular expression can be used. Some useful examples are listed below   | Input text | Regex | Result | | ---------- | ----- | ------ | | price- 10.99€ | <code>[0-9]+\\.[0-9]+</code> | 10.99 | | phone- 0 (944) 244-18-22 | <code>\\w+</code> | 09442441822 |   *** ### Details. Chaining. The Link field type serves as a navigation link to a details page containing more data. A special _Path_ option is used for navigation only. When the Path option specified, no results from the current page returned. But grouped results from details pages will be pulled instead. You can use chaining functionality of Dataflow Kit scraper to retrieve all the detail page data at the same time.  (required)
        :type parserequest: Parserequest
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._parse_serialize(
            parserequest=parserequest,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "object",
            '400': None,
            '401': None,
            '500': None,
        }
        response_data = self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        return response_data.response


    def _parse_serialize(
        self,
        parserequest,
        _request_auth,
        _content_type,
        _headers,
        _host_index,
    ) -> RequestSerialized:

        _host = None

        _collection_formats: Dict[str, str] = {
        }

        _path_params: Dict[str, str] = {}
        _query_params: List[Tuple[str, str]] = []
        _header_params: Dict[str, Optional[str]] = _headers or {}
        _form_params: List[Tuple[str, str]] = []
        _files: Dict[
            str, Union[str, bytes, List[str], List[bytes], List[Tuple[str, bytes]]]
        ] = {}
        _body_params: Optional[bytes] = None

        # process the path parameters
        # process the query parameters
        # process the header parameters
        # process the form parameters
        # process the body parameter
        if parserequest is not None:
            _body_params = parserequest


        # set the HTTP header `Accept`
        if 'Accept' not in _header_params:
            _header_params['Accept'] = self.api_client.select_header_accept(
                [
                    'application/json', 
                    'text/plain; charset=utf-8'
                ]
            )

        # set the HTTP header `Content-Type`
        if _content_type:
            _header_params['Content-Type'] = _content_type
        else:
            _default_content_type = (
                self.api_client.select_header_content_type(
                    [
                        'application/json'
                    ]
                )
            )
            if _default_content_type is not None:
                _header_params['Content-Type'] = _default_content_type

        # authentication setting
        _auth_settings: List[str] = [
            'ApiKeyAuth'
        ]

        return self.api_client.param_serialize(
            method='POST',
            resource_path='/parse',
            path_params=_path_params,
            query_params=_query_params,
            header_params=_header_params,
            body=_body_params,
            post_params=_form_params,
            files=_files,
            auth_settings=_auth_settings,
            collection_formats=_collection_formats,
            _host=_host,
            _request_auth=_request_auth
        )


